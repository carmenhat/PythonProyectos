#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script unificado para extracciÃ³n, enriquecimiento y normalizaciÃ³n de datos sobre pelÃ­culas
de la secciÃ³n oficial del Festival de Cannes (2015-2023).

Este script combina funcionalidad de varios scripts previos para extraer datos de pelÃ­culas desde Wikipedia,
IMDb y enriquecer la informaciÃ³n con productoras y paÃ­ses de origen. AdemÃ¡s, normaliza los nombres de las productoras
para evitar duplicados y errores de escritura.
El script realiza las siguientes tareas:
1. Extrae la lista de pelÃ­culas desde Wikipedia.
2. Enriquecer la informaciÃ³n con productoras desde Wikipedia.
3. Busca IDs de IMDb y extrae informaciÃ³n de productoras y paÃ­ses.
4. Normaliza los nombres de las productoras.
5. Consolida la informaciÃ³n en un DataFrame y la guarda en un archivo Excel.
6. Agrupa compaÃ±Ã­as similares en toda la base de datos.
7. Guarda el resultado en un archivo Excel.
8. Maneja errores y excepciones durante el proceso de scraping y enriquecimiento.
9. Utiliza un normalizador de nombres de productoras para evitar duplicados y errores de escritura.
10. AÃ±ade emojis de banderas para los paÃ­ses de origen de las pelÃ­culas.
11. Permite la configuraciÃ³n de aÃ±os a extraer y URLs base para Wikipedia.
12. Utiliza BeautifulSoup para el scraping de datos y pandas para la manipulaciÃ³n de datos.
nECESITA instalar las librerÃ­as: requests, pandas, beautifulsoup4, openpyxl y company_normalizer
company_normalizer es un mÃ³dulo externo que debe estar en la misma carpeta que este script.: lo he creado para normalizar los nombres de las productoras.

Autor: carmenhat
Fecha: Abril 2025
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import os
from urllib.parse import quote
from pathlib import Path
from company_normalizer import ProductionCompanyNormalizer  # Importamos el normalizador (definido en segundo archivo)

# Constantes
YEARS = list(range(2015, 2024))
BASE_WIKI_URL = "https://en.wikipedia.org/wiki/{}"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9"
}

# Lista de paÃ­ses con emojis de banderas
COUNTRY_EMOJIS = {
    "Spain": "ğŸ‡ªğŸ‡¸ Spain",
    "EspaÃ±a": "ğŸ‡ªğŸ‡¸ Spain",
    "France": "ğŸ‡«ğŸ‡· France",
    "Francia": "ğŸ‡«ğŸ‡· France",
    "USA": "ğŸ‡ºğŸ‡¸ USA",
    "United States": "ğŸ‡ºğŸ‡¸ USA",
    "Estados Unidos": "ğŸ‡ºğŸ‡¸ USA",
    "Italy": "ğŸ‡®ğŸ‡¹ Italy",
    "Italia": "ğŸ‡®ğŸ‡¹ Italy",
    "Japan": "ğŸ‡¯ğŸ‡µ Japan",
    "JapÃ³n": "ğŸ‡¯ğŸ‡µ Japan",
    "South Korea": "ğŸ‡°ğŸ‡· South Korea",
    "Corea del Sur": "ğŸ‡°ğŸ‡· South Korea",
    "United Kingdom": "ğŸ‡¬ğŸ‡§ United Kingdom",
    "Reino Unido": "ğŸ‡¬ğŸ‡§ United Kingdom",
    "UK": "ğŸ‡¬ğŸ‡§ United Kingdom",
    "Mexico": "ğŸ‡²ğŸ‡½ Mexico",
    "MÃ©xico": "ğŸ‡²ğŸ‡½ Mexico",
    "Canada": "ğŸ‡¨ğŸ‡¦ Canada",
    "CanadÃ¡": "ğŸ‡¨ğŸ‡¦ Canada",
    "Denmark": "ğŸ‡©ğŸ‡° Denmark",
    "Dinamarca": "ğŸ‡©ğŸ‡° Denmark",
    "Switzerland": "ğŸ‡¨ğŸ‡­ Switzerland",
    "Suiza": "ğŸ‡¨ğŸ‡­ Switzerland",
    "Turkey": "ğŸ‡¹ğŸ‡· Turkey",
    "TurquÃ­a": "ğŸ‡¹ğŸ‡· Turkey",
    "Iran": "ğŸ‡®ğŸ‡· Iran",
    "IrÃ¡n": "ğŸ‡®ğŸ‡· Iran",
    "Germany": "ğŸ‡©ğŸ‡ª Germany",
    "Alemania": "ğŸ‡©ğŸ‡ª Germany",
    "Austria": "ğŸ‡¦ğŸ‡¹ Austria",
    "Belgium": "ğŸ‡§ğŸ‡ª Belgium",
    "BÃ©lgica": "ğŸ‡§ğŸ‡ª Belgium",
    "Brazil": "ğŸ‡§ğŸ‡· Brazil",
    "Brasil": "ğŸ‡§ğŸ‡· Brazil",
    "China": "ğŸ‡¨ğŸ‡³ China",
    "Russia": "ğŸ‡·ğŸ‡º Russia",
    "Rusia": "ğŸ‡·ğŸ‡º Russia",
    "Sweden": "ğŸ‡¸ğŸ‡ª Sweden",
    "Suecia": "ğŸ‡¸ğŸ‡ª Sweden"
}

def clean_movie_title(title):
    """
    Limpia el tÃ­tulo de la pelÃ­cula eliminando texto entre parÃ©ntesis 
    que no sea un aÃ±o (de 4 dÃ­gitos).
    """
    # Primero conservamos cualquier aÃ±o entre parÃ©ntesis
    years = re.findall(r'\((\d{4})\)', title)
    
    # Eliminar todos los parÃ©ntesis y su contenido
    clean_title = re.sub(r'\([^)]*\)', '', title)
    
    # Limpiar espacios extra
    clean_title = clean_title.strip()
    
    return clean_title

def extract_films_from_wiki():
    """
    Extrae la lista de pelÃ­culas del Festival de Cannes entre 2015-2023 desde Wikipedia.
    
    Returns:
        DataFrame con informaciÃ³n bÃ¡sica de las pelÃ­culas
    """
    print("ğŸŒ Iniciando extracciÃ³n de pelÃ­culas de Wikipedia...")
    
    data = []
    
    for year in YEARS:
        print(f"\nProcesando {year}...")
        wiki_title = f"{year}_Cannes_Film_Festival"
        url = BASE_WIKI_URL.format(wiki_title)
        
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()  # Levantar excepciÃ³n si hay error HTTP
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ Error al acceder a {url}: {e}")
            continue
        
        soup = BeautifulSoup(response.text, "html.parser")
        tables = soup.find_all("table", class_="wikitable")
        
        if not tables:
            print(f"âš ï¸ No se encontraron tablas relevantes en {url}")
            continue
        
        tablas_validas = 0
        
        for table in tables:
            headers = [th.get_text(strip=True).lower() for th in table.find_all("th")]
            
            # Verificar si la tabla contiene datos de pelÃ­culas
            if not any("film" in h or "title" in h for h in headers):
                continue
            
            tablas_validas += 1
            
            for row in table.find_all("tr")[1:]:
                cols = row.find_all(["td", "th"])
                if len(cols) < 2:
                    continue
                
                # Extraer tÃ­tulo de la pelÃ­cula
                film_elem = cols[0]
                film = film_elem.get_text(strip=True)
                
                # Intentar extraer enlace a Wikipedia para la pelÃ­cula
                film_link = film_elem.find("a")
                film_wiki_url = ""
                if film_link and "href" in film_link.attrs:
                    href = film_link["href"]
                    if href.startswith("/wiki/"):
                        film_wiki_url = f"https://en.wikipedia.org{href}"
                        
                # Si no hay enlace directo, construir uno basado en el tÃ­tulo
                if not film_wiki_url:
                    film_url = film.replace(" ", "_")
                    film_wiki_url = f"https://en.wikipedia.org/wiki/{film_url}"
                
                # Extraer director
                director = cols[1].get_text(strip=True) if len(cols) > 1 else ""
                
                # Buscar columna de paÃ­ses si existe
                countries = ""
                for i, h in enumerate(headers):
                    if "country" in h and i < len(cols):
                        countries = cols[i].get_text(strip=True)
                
                # Identificar paÃ­s (EspaÃ±a, Francia o USA)
                country_flag = ""
                for country_key, emoji_country in COUNTRY_EMOJIS.items():
                    if country_key in countries:
                        if not country_flag:  # AÃ±adimos el primero que encontremos
                            country_flag = emoji_country
                        else:  # Si ya hay uno, aÃ±adimos coma y el nuevo
                            country_flag += f", {emoji_country}"
                
                # Extraer productoras si existe una columna relevante
                production_company = ""
                for i, h in enumerate(headers):
                    if ("production" in h or "studio" in h) and i < len(cols):
                        production_company = cols[i].get_text(strip=True)
                
                # AÃ±adir datos a la lista
                data.append({
                    "year": year,
                    "title": film,
                    "director": director,
                    "countries": countries,
                    "section": "Official Selection (Wikipedia)",
                    "country_emoji": country_flag,
                    "production_company_wiki": production_company,
                    "film_wiki_url": film_wiki_url
                })
        
        if tablas_validas == 0:
            print(f"âš ï¸ No se encontrÃ³ ninguna tabla con tÃ­tulos de pelÃ­culas en {url}")
        
        # Pausa para no sobrecargar el servidor
        time.sleep(1)
    
    # Crear DataFrame con todos los datos recopilados
    if data:
        films_df = pd.DataFrame(data)
        print(f"âœ… Se extrajeron datos de {len(films_df)} pelÃ­culas")
        return films_df
    else:
        print("âŒ No se encontraron datos de pelÃ­culas")
        return pd.DataFrame()

def extract_wiki_production_companies(df):
    """
    Enriquece el DataFrame con las productoras extraÃ­das de las pÃ¡ginas de Wikipedia
    de cada pelÃ­cula.
    
    Args:
        df: DataFrame con los datos de las pelÃ­culas (debe contener 'film_wiki_url')
        
    Returns:
        DataFrame actualizado con informaciÃ³n de productoras
    """
    print("\nğŸ” Extrayendo productoras desde Wikipedia...")
    
    # Verificar que exista la columna necesaria
    if "film_wiki_url" not in df.columns:
        print("âŒ Error: No existe la columna 'film_wiki_url'")
        return df
    
    # Columna para almacenar las productoras encontradas en Wikipedia
    df["production_company_wiki_page"] = ""
    
    for i, row in df.iterrows():
        url = row["film_wiki_url"]
        print(f"ğŸ” Procesando {row['title']} ({row['year']})")
        
        try:
            r = requests.get(url, headers=HEADERS, timeout=10)
            r.raise_for_status()
        except Exception as e:
            print(f"âŒ No se pudo acceder a {url}: {e}")
            continue
        
        soup = BeautifulSoup(r.text, "html.parser")
        
        # Buscar en el infobox de la pelÃ­cula
        infobox = soup.find("table", class_="infobox vevent")
        prod = ""
        
        if infobox:
            for row in infobox.find_all("tr"):
                th = row.find("th")
                td = row.find("td")
                if th and td:
                    label = th.get_text(strip=True).lower()
                    if "production" in label or "studio" in label or "productora" in label:
                        prod = td.get_text(separator=", ", strip=True)
                        break
        
        if prod:
            df.at[i, "production_company_wiki_page"] = prod
        
        # Pausa para no sobrecargar el servidor
        time.sleep(1)
    
    return df

def search_imdb_id(title, year=None):
    """
    Busca una pelÃ­cula en IMDb y devuelve su ID.
    
    Args:
        title: TÃ­tulo de la pelÃ­cula
        year: AÃ±o de la pelÃ­cula (opcional)
        
    Returns:
        ID de IMDb o None si no se encuentra
    """
    # Limpiar el tÃ­tulo para eliminar texto adicional
    clean_title = clean_movie_title(title)
    
    search_query = clean_title
    if year:
        search_query += f" {year}"
    
    encoded_query = quote(search_query)
    search_url = f"https://www.imdb.com/find/?q={encoded_query}&s=tt&exact=true&ref_=fn_tt_ex"
    
    try:
        response = requests.get(search_url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Buscar resultados de pelÃ­culas
        results = soup.select("li.find-title-result")
        
        for result in results:
            # Extraer tÃ­tulo y aÃ±o
            title_elem = result.select_one(".ipc-metadata-list-summary-item__t")
            if not title_elem:
                continue
                
            result_title = title_elem.text.strip()
            
            # Extraer aÃ±o si estÃ¡ disponible
            year_match = re.search(r'\((\d{4})\)', result.text)
            result_year = int(year_match.group(1)) if year_match else None
            
            # Verificar coincidencia - usar el tÃ­tulo limpio para comparaciÃ³n
            if (not year or not result_year or abs(int(year) - result_year) <= 1) and \
               (result_title.lower() in clean_title.lower() or clean_title.lower() in result_title.lower()):
                
                # Extraer ID de IMDb
                link = title_elem.get("href", "")
                imdb_id_match = re.search(r'/title/(tt\d+)/', link)
                if imdb_id_match:
                    return imdb_id_match.group(1)
        
        # Si no encontramos coincidencia exacta, probar con el primer resultado
        if results:
            first_result = results[0]
            link = first_result.select_one("a[href*='/title/']")
            if link:
                imdb_id_match = re.search(r'/title/(tt\d+)/', link.get("href", ""))
                if imdb_id_match:
                    return imdb_id_match.group(1)
                    
        return None
        
    except Exception as e:
        print(f"Error buscando '{clean_title}' en IMDb: {e}")
        return None

def scrape_imdb_for_production_companies(imdb_id):
    """
    Extrae informaciÃ³n de compaÃ±Ã­as productoras desde IMDb.
    
    Args:
        imdb_id: ID de IMDb de la pelÃ­cula
        
    Returns:
        Lista de nombres de compaÃ±Ã­as productoras
    """
    if not imdb_id:
        return []
        
    url = f"https://www.imdb.com/title/{imdb_id}/companycredits"
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        production_companies = []
        
        # Buscar la secciÃ³n "Production Companies"
        production_section = None
        
        # MÃ©todo 1: buscar por encabezado
        for header in soup.find_all(["h2", "h3", "h4"]):
            if "Production" in header.text and "Companies" in header.text:
                production_section = header.find_next("ul")
                break
                
        # MÃ©todo 2: buscar por ID o clase especÃ­fica
        if not production_section:
            production_section = soup.select_one("#production")
            
        # MÃ©todo 3: buscar por la estructura general
        if not production_section:
            sections = soup.select(".ipc-metadata-list")
            for section in sections:
                header = section.select_one(".ipc-metadata-list-item__label")
                if header and "Production compan" in header.text.lower():
                    production_section = section
                    break
        
        # Extraer compaÃ±Ã­as
        if production_section:
            company_items = production_section.select("li")
            
            if not company_items:  # Estructura alternativa
                company_items = production_section.select(".ipc-metadata-list-item")
            
            for item in company_items:
                company_link = item.select_one("a")
                if company_link:
                    company_name = company_link.text.strip()
                    production_companies.append(company_name)
        
        return production_companies
        
    except Exception as e:
        print(f"Error obteniendo productoras para {imdb_id}: {e}")
        return []

def scrape_imdb_for_countries(imdb_id):
    """
    Extrae informaciÃ³n de paÃ­ses desde IMDb.
    
    Args:
        imdb_id: ID de IMDb de la pelÃ­cula
        
    Returns:
        Lista de paÃ­ses
    """
    if not imdb_id:
        return []
        
    url = f"https://www.imdb.com/title/{imdb_id}/"
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        countries = []
        
        # MÃ©todo 1: Buscar en los metadatos principales
        metadata_blocks = soup.select(".ipc-metadata-list")
        for block in metadata_blocks:
            header = block.select_one(".ipc-metadata-list-item__label")
            if header and ("Countries of origin" in header.text or "Country of origin" in header.text):
                country_elements = block.select(".ipc-metadata-list-item__list-content-item")
                for element in country_elements:
                    countries.append(element.text.strip())
                break
        
        # MÃ©todo 2: Buscar en la informaciÃ³n tÃ©cnica si estÃ¡ disponible
        if not countries:
            details_page = f"https://www.imdb.com/title/{imdb_id}/technical/"
            try:
                details_response = requests.get(details_page, headers=HEADERS, timeout=10)
                details_response.raise_for_status()
                details_soup = BeautifulSoup(details_response.text, "html.parser")
                
                for item in details_soup.select(".technical-list li"):
                    label = item.select_one("h4")
                    if label and "Country" in label.text:
                        value = item.select_one("div")
                        if value:
                            for country in value.text.split(","):
                                countries.append(country.strip())
            except Exception as e:
                print(f"Error obteniendo detalles tÃ©cnicos para {imdb_id}: {e}")
        
        # MÃ©todo 3: Buscar en la secciÃ³n "Details" de la pÃ¡gina principal
        if not countries:
            details_section = soup.select_one("[data-testid='title-details-section']")
            if details_section:
                country_item = details_section.find(lambda tag: tag.name == "li" and "Country" in tag.text)
                if country_item:
                    country_links = country_item.select("a")
                    for link in country_links:
                        countries.append(link.text.strip())
        
        return countries
        
    except Exception as e:
        print(f"Error obteniendo paÃ­ses para {imdb_id}: {e}")
        return []

def enrich_with_imdb_data(df):
    """
    Enriquece el DataFrame con datos de IMDb: IDs, productoras y paÃ­ses.
    
    Args:
        df: DataFrame con los datos de las pelÃ­culas
        
    Returns:
        DataFrame actualizado con informaciÃ³n de IMDb
    """
    print("\nğŸ¬ Enriqueciendo con datos de IMDb...")
    
    # AÃ±adir columnas para IMDb si no existen
    if "imdb_id" not in df.columns:
        df["imdb_id"] = None
    if "imdb_production_companies" not in df.columns:
        df["imdb_production_companies"] = None
    if "imdb_countries" not in df.columns:
        df["imdb_countries"] = None
    
    # Procesar cada pelÃ­cula
    for i, row in df.iterrows():
        # Obtener el tÃ­tulo original y mostrar tambiÃ©n el tÃ­tulo limpio
        original_title = row['title']
        clean_title = clean_movie_title(original_title)
        
        print(f"\nğŸ“½ï¸ Procesando {original_title} ({row['year']})...")
        if original_title != clean_title:
            print(f"   â†’ TÃ­tulo limpio para bÃºsqueda: '{clean_title}'")
        
        # Buscar ID de IMDb si no existe
        if pd.isna(df.at[i, "imdb_id"]) or df.at[i, "imdb_id"] == "":
            imdb_id = search_imdb_id(original_title, row["year"])
            
            if imdb_id:
                print(f"âœ… ID de IMDb encontrado: {imdb_id}")
                df.at[i, "imdb_id"] = imdb_id
                
                # Obtener productoras
                companies = scrape_imdb_for_production_companies(imdb_id)
                
                if companies:
                    companies_str = ", ".join(companies)
                    print(f"ğŸ¢ Productoras: {companies_str}")
                    df.at[i, "imdb_production_companies"] = companies_str
                else:
                    print("âŒ No se encontraron productoras")
                
                # Obtener paÃ­ses
                countries = scrape_imdb_for_countries(imdb_id)
                
                if countries:
                    # Convertir a formato con emoji si es posible
                    countries_with_emoji = []
                    for country in countries:
                        added = False
                        for target, emoji_name in COUNTRY_EMOJIS.items():
                            if target in country:
                                countries_with_emoji.append(emoji_name)
                                added = True
                                break
                        
                        # Si no encontramos emoji para el paÃ­s, lo aÃ±adimos tal cual
                        if not added:
                            countries_with_emoji.append(country)
                    
                    # Guardar paÃ­ses en formato string
                    countries_str = ", ".join(countries)
                    emoji_countries_str = ", ".join(countries_with_emoji)
                    
                    print(f"ğŸŒ PaÃ­ses: {countries_str}")
                    df.at[i, "imdb_countries"] = countries_str
                    
                    # Actualizar columna country_emoji si estÃ¡ vacÃ­a o incompleta
                    if pd.isna(df.at[i, "country_emoji"]) or df.at[i, "country_emoji"] == "":
                        df.at[i, "country_emoji"] = emoji_countries_str
                    # Si ya tiene datos, aÃ±adir solo los que faltan
                    else:
                        existing = set(df.at[i, "country_emoji"].split(", "))
                        new = set(emoji_countries_str.split(", "))
                        combined = existing.union(new)
                        df.at[i, "country_emoji"] = ", ".join(combined)
                        
                else:
                    print("âŒ No se encontraron datos de paÃ­ses")
            else:
                print("âŒ No se encontrÃ³ ID de IMDb")
        
        # Esperar para no sobrecargar el servidor
        time.sleep(2)
    
    return df

def consolidate_production_companies(df, normalizer):
    """
    Consolida todas las fuentes de compaÃ±Ã­as productoras en una sola columna
    y las normaliza.
    
    Args:
        df: DataFrame con los datos
        normalizer: Instancia de ProductionCompanyNormalizer
        
    Returns:
        DataFrame actualizado con columna consolidada y normalizada
    """
    print("\nğŸ”„ Consolidando y normalizando nombres de productoras...")
    
    # Columnas que pueden contener informaciÃ³n de productoras
    company_columns = [
        'production_company_wiki', 
        'production_company_wiki_page', 
        'imdb_production_companies'
    ]
    
    # Verificar quÃ© columnas existen en el DataFrame
    available_columns = [col for col in company_columns if col in df.columns]
    
    if not available_columns:
        print("âŒ No se encontraron columnas con datos de productoras")
        return df
        
    # Crear columna consolidada
    df['productoras_consolidadas'] = None
    
    # Consolidar productoras de todas las fuentes
    for i, row in df.iterrows():
        all_companies = set()
        
        for col in available_columns:
            if not pd.isna(row[col]) and row[col] != "":
                companies = [c.strip() for c in str(row[col]).split(',')]
                all_companies.update([c for c in companies if c])
        
        if all_companies:
            df.at[i, 'productoras_consolidadas'] = ", ".join(all_companies)
    
    # Normalizar cada compaÃ±Ã­a productora
    df['productoras_normalizadas'] = None
    
    for i, row in df.iterrows():
        if pd.isna(row['productoras_consolidadas']) or row['productoras_consolidadas'] == "":
            continue
            
        # Dividir por comas y normalizar cada compaÃ±Ã­a
        companies = [c.strip() for c in str(row['productoras_consolidadas']).split(',')]
        normalized_companies = [normalizer.normalize(c) for c in companies if c]
        
        # Eliminar duplicados
        normalized_companies = list(dict.fromkeys(normalized_companies))
        
        # Unir de nuevo con comas
        df.at[i, 'productoras_normalizadas'] = ", ".join(normalized_companies)
    
    # Agrupar compaÃ±Ã­as similares en toda la base de datos
    print("ğŸ”„ Agrupando compaÃ±Ã­as similares en todo el dataset...")
    
    # Extraer todas las compaÃ±Ã­as Ãºnicas
    all_companies = set()
    for value in df['productoras_normalizadas'].dropna():
        if isinstance(value, str):
            companies = [c.strip() for c in value.split(',')]
            all_companies.update([c for c in companies if c])
    
    # Agrupar compaÃ±Ã­as similares
    clusters = normalizer.cluster_similar_companies(all_companies)
    
    # Crear mapa de reemplazo
    replacement_map = {}
    for canonical, variants in clusters.items():
        for variant in variants:
            if variant != canonical:
                replacement_map[variant] = canonical
    
    # Aplicar reemplazos a la columna normalizada
    for i, row in df.iterrows():
        if pd.isna(df.at[i, 'productoras_normalizadas']):
            continue
            
        companies = [c.strip() for c in df.at[i, 'productoras_normalizadas'].split(',')]
        replaced_companies = [replacement_map.get(c, c) for c in companies]
        
        # Eliminar duplicados que pudieron surgir de la normalizaciÃ³n
        replaced_companies = list(dict.fromkeys(replaced_companies))
        
        df.at[i, 'productoras_normalizadas'] = ", ".join(replaced_companies)
    
    return df

def main():
    """FunciÃ³n principal que coordina todo el proceso."""
    try:
        print("ğŸš€ Iniciando proceso unificado de extracciÃ³n de datos de Cannes...")
        
        # Paso 1: ExtracciÃ³n inicial desde Wikipedia
        films_df = extract_films_from_wiki()
        
        if films_df.empty:
            print("âŒ No se pudieron extraer datos. Fin del proceso.")
            return
        
        # Paso 2: Enriquecer con productoras desde Wikipedia
        films_df = extract_wiki_production_companies(films_df)
        
        # Paso 3: Enriquecer con datos de IMDb (IDs, productoras y paÃ­ses)
        films_df = enrich_with_imdb_data(films_df)
        
        # Paso 4: Normalizar productoras
        normalizer = ProductionCompanyNormalizer()
        films_df = consolidate_production_companies(films_df, normalizer)
        
        # Crear directorio para datos si no existe
        output_dir = Path("datos_generados")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Guardar resultados
        output_file = output_dir / "cannes_dataset_unificado.xlsx"
        films_df.to_excel(output_file, index=False)
        print(f"\nâœ… Proceso completado. Datos guardados en '{output_file.resolve()}'")
        
    except Exception as e:
        print(f"\nâŒ Error en el procesamiento: {e}")

if __name__ == "__main__":
    main()
